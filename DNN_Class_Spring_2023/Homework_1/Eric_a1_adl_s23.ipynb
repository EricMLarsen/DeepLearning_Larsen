{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#Import the Required Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.tree\n",
    "import sklearn.neighbors\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.0              0.27         0.36            20.7      0.045   \n1            6.3              0.30         0.34             1.6      0.049   \n2            8.1              0.28         0.40             6.9      0.050   \n3            7.2              0.23         0.32             8.5      0.058   \n4            7.2              0.23         0.32             8.5      0.058   \n\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                 45.0                 170.0   1.0010  3.00       0.45   \n1                 14.0                 132.0   0.9940  3.30       0.49   \n2                 30.0                  97.0   0.9951  3.26       0.44   \n3                 47.0                 186.0   0.9956  3.19       0.40   \n4                 47.0                 186.0   0.9956  3.19       0.40   \n\n   alcohol  quality  \n0      8.8        6  \n1      9.5        6  \n2     10.1        6  \n3      9.9        6  \n4      9.9        6  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.0</td>\n      <td>0.27</td>\n      <td>0.36</td>\n      <td>20.7</td>\n      <td>0.045</td>\n      <td>45.0</td>\n      <td>170.0</td>\n      <td>1.0010</td>\n      <td>3.00</td>\n      <td>0.45</td>\n      <td>8.8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.3</td>\n      <td>0.30</td>\n      <td>0.34</td>\n      <td>1.6</td>\n      <td>0.049</td>\n      <td>14.0</td>\n      <td>132.0</td>\n      <td>0.9940</td>\n      <td>3.30</td>\n      <td>0.49</td>\n      <td>9.5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.1</td>\n      <td>0.28</td>\n      <td>0.40</td>\n      <td>6.9</td>\n      <td>0.050</td>\n      <td>30.0</td>\n      <td>97.0</td>\n      <td>0.9951</td>\n      <td>3.26</td>\n      <td>0.44</td>\n      <td>10.1</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read in the data from the wine file display for verification\n",
    "\n",
    "winedf = pd.read_csv(\"Data/winequality-white.csv\",delimiter= \";\")\n",
    "winedf.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.0              0.27         0.36            20.7      0.045   \n1            6.3              0.30         0.34             1.6      0.049   \n2            8.1              0.28         0.40             6.9      0.050   \n3            7.2              0.23         0.32             8.5      0.058   \n4            7.2              0.23         0.32             8.5      0.058   \n\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                 45.0                 170.0   1.0010  3.00       0.45   \n1                 14.0                 132.0   0.9940  3.30       0.49   \n2                 30.0                  97.0   0.9951  3.26       0.44   \n3                 47.0                 186.0   0.9956  3.19       0.40   \n4                 47.0                 186.0   0.9956  3.19       0.40   \n\n   alcohol  quality  Target  \n0      8.8        6       1  \n1      9.5        6       1  \n2     10.1        6       1  \n3      9.9        6       1  \n4      9.9        6       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n      <th>Target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.0</td>\n      <td>0.27</td>\n      <td>0.36</td>\n      <td>20.7</td>\n      <td>0.045</td>\n      <td>45.0</td>\n      <td>170.0</td>\n      <td>1.0010</td>\n      <td>3.00</td>\n      <td>0.45</td>\n      <td>8.8</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.3</td>\n      <td>0.30</td>\n      <td>0.34</td>\n      <td>1.6</td>\n      <td>0.049</td>\n      <td>14.0</td>\n      <td>132.0</td>\n      <td>0.9940</td>\n      <td>3.30</td>\n      <td>0.49</td>\n      <td>9.5</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8.1</td>\n      <td>0.28</td>\n      <td>0.40</td>\n      <td>6.9</td>\n      <td>0.050</td>\n      <td>30.0</td>\n      <td>97.0</td>\n      <td>0.9951</td>\n      <td>3.26</td>\n      <td>0.44</td>\n      <td>10.1</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.2</td>\n      <td>0.23</td>\n      <td>0.32</td>\n      <td>8.5</td>\n      <td>0.058</td>\n      <td>47.0</td>\n      <td>186.0</td>\n      <td>0.9956</td>\n      <td>3.19</td>\n      <td>0.40</td>\n      <td>9.9</td>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set up target value depending on the quality value\n",
    "\n",
    "#if quality in range 0,5 then 0 else 1, add target column to dataframe\n",
    "def target_set_value(row):\n",
    "    if row['quality'] >= 0 and row['quality'] <=5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "winedf['Target'] = winedf.apply(target_set_value,axis = 1)\n",
    "winedf.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Case Count [Target = 1] = 3258\n",
      "Negative Case Count [Target = 0] = 1640\n",
      "Total Case Count = 4898\n",
      "Imbalance Ratio of DataSet ([Positive Case Count]/[Negative Case Count]): = 1.9866\n",
      "Percent Positive Cases: 66.5\n",
      "Percent Negative Cases: 33.5\n"
     ]
    }
   ],
   "source": [
    "#Write the imbalance ratio\n",
    "\n",
    "#get the number of each class\n",
    "PositiveClassCount = winedf.loc[winedf.Target == 1,'Target'].count()\n",
    "NegativeClassCount = winedf.loc[winedf.Target == 0,'Target'].count()\n",
    "TotalClassCount = PositiveClassCount+NegativeClassCount\n",
    "print(f\"Positive Case Count [Target = 1] = {PositiveClassCount}\")\n",
    "print(f\"Negative Case Count [Target = 0] = {NegativeClassCount}\")\n",
    "print(f\"Total Case Count = {PositiveClassCount+NegativeClassCount}\")\n",
    "print(f\"Imbalance Ratio of DataSet ([Positive Case Count]/[Negative Case Count]): = {round(PositiveClassCount/NegativeClassCount,4)}\")\n",
    "print(f\"Percent Positive Cases: {round(PositiveClassCount/TotalClassCount,3)*100}\")\n",
    "print(f\"Percent Negative Cases: {round(NegativeClassCount/TotalClassCount,3)*100}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(0.25) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m RandomState \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m13\u001B[39m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m#Split the Xvalues and Yvalues into the desired subsets of the datapool\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m x_train, x_test, y_train, y_test \u001B[38;5;241m=\u001B[39m  \u001B[43mmodel_selection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43mYvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRandomState\u001B[49m\u001B[43m,\u001B[49m\u001B[43mstratify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.25\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m#Split X_Train values again to produce the validation set for the datapool\u001B[39;00m\n\u001B[0;32m     15\u001B[0m x_train, x_val, y_train, y_val \u001B[38;5;241m=\u001B[39m model_selection\u001B[38;5;241m.\u001B[39mtrain_test_split(x_train,y_train,test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m,random_state\u001B[38;5;241m=\u001B[39mRandomState,stratify\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2583\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2579\u001B[0m         CVClass \u001B[38;5;241m=\u001B[39m ShuffleSplit\n\u001B[0;32m   2581\u001B[0m     cv \u001B[38;5;241m=\u001B[39m CVClass(test_size\u001B[38;5;241m=\u001B[39mn_test, train_size\u001B[38;5;241m=\u001B[39mn_train, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[1;32m-> 2583\u001B[0m     train, test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marrays\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstratify\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   2585\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   2586\u001B[0m     chain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n\u001B[0;32m   2587\u001B[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m arrays\n\u001B[0;32m   2588\u001B[0m     )\n\u001B[0;32m   2589\u001B[0m )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2160\u001B[0m, in \u001B[0;36mStratifiedShuffleSplit.split\u001B[1;34m(self, X, y, groups)\u001B[0m\n\u001B[0;32m   2126\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msplit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y, groups\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   2127\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001B[39;00m\n\u001B[0;32m   2128\u001B[0m \n\u001B[0;32m   2129\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2158\u001B[0m \u001B[38;5;124;03m    to an integer.\u001B[39;00m\n\u001B[0;32m   2159\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2160\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43my\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   2161\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msplit(X, y, groups)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:929\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    921\u001B[0m         _assert_all_finite(\n\u001B[0;32m    922\u001B[0m             array,\n\u001B[0;32m    923\u001B[0m             input_name\u001B[38;5;241m=\u001B[39minput_name,\n\u001B[0;32m    924\u001B[0m             estimator_name\u001B[38;5;241m=\u001B[39mestimator_name,\n\u001B[0;32m    925\u001B[0m             allow_nan\u001B[38;5;241m=\u001B[39mforce_all_finite \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow-nan\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    926\u001B[0m         )\n\u001B[0;32m    928\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_samples \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 929\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m \u001B[43m_num_samples\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    930\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_samples \u001B[38;5;241m<\u001B[39m ensure_min_samples:\n\u001B[0;32m    931\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    932\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m sample(s) (shape=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m) while a\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    933\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m minimum of \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m is required\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    934\u001B[0m             \u001B[38;5;241m%\u001B[39m (n_samples, array\u001B[38;5;241m.\u001B[39mshape, ensure_min_samples, context)\n\u001B[0;32m    935\u001B[0m         )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:335\u001B[0m, in \u001B[0;36m_num_samples\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    334\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 335\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    336\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSingleton array \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m cannot be considered a valid collection.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m x\n\u001B[0;32m    337\u001B[0m         )\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;66;03m# Check that shape is returning an integer or default to len\u001B[39;00m\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;66;03m# Dask dataframes may not return numeric shape[0] value\u001B[39;00m\n\u001B[0;32m    340\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], numbers\u001B[38;5;241m.\u001B[39mIntegral):\n",
      "\u001B[1;31mTypeError\u001B[0m: Singleton array array(0.25) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "#Split the data into Train, Validation, Test sets (60,20,20) data split\n",
    "\n",
    "#Set up the input values of the system, all columns of the data frame except the quality and target values\n",
    "Xvalues = winedf.drop(columns=['quality','Target'])\n",
    "#Set up the desired output values of the systme, the Target value column of the data frame\n",
    "Yvalues = winedf.Target\n",
    "\n",
    "#Set Up random state value to be able to reproduce the results\n",
    "RandomState = 13\n",
    "\n",
    "#Split the Xvalues and Yvalues into the desired subsets of the datapool\n",
    "x_train, x_test, y_train, y_test =  model_selection.train_test_split(Xvalues,Yvalues,test_size=0.2,random_state=RandomState,stratify=0.25)\n",
    "\n",
    "#Split X_Train values again to produce the validation set for the datapool\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(x_train,y_train,test_size=0.25,random_state=RandomState,stratify=0.25)\n",
    "\n",
    "#display size of each portion\n",
    "print(f\"Number of Train Instances: {len(x_train)}\")\n",
    "print(f\"Number of Validation Instances: {len(x_val)}\")\n",
    "print(f\"Number of Test Instances: {len(x_test)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do Z Normalization on the Training, Validation, and Testing Data sets, but not to the classification classes\n",
    "#define the function to be used\n",
    "def Z_Normalization(arr):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(arr)\n",
    "    scaler.mean_\n",
    "    new_arr = scaler.transform(arr)\n",
    "    return new_arr\n",
    "#Train Data\n",
    "X_train_z = Z_Normalization(x_train)\n",
    "Y_train_z = y_train\n",
    "#Validation data\n",
    "X_val_z = Z_Normalization(x_val)\n",
    "Y_val_z = y_val\n",
    "#Test data\n",
    "X_test_z = Z_Normalization(x_test)\n",
    "Y_test_z = y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sklearn KNN model\n",
    "def KNN_model(x,y,N,x_val,y_val,display):\n",
    "    model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=N)\n",
    "    model.fit(x,y)\n",
    "    #Test model on validation data\n",
    "    sklearn.neighbors.KNeighborsClassifier(n_neighbors = N)\n",
    "    prediction = model.predict(x_val)\n",
    "    true = y_val\n",
    "    acc = (sklearn.metrics.accuracy_score(true,prediction))\n",
    "    precision = (sklearn.metrics.precision_score(true,prediction))\n",
    "    recall = (sklearn.metrics.recall_score(true,prediction))\n",
    "    f1 = (sklearn.metrics.f1_score(true,prediction))\n",
    "    cm = sklearn.metrics.confusion_matrix(true,prediction,labels=model.classes_)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(cm,display_labels=model.classes_)\n",
    "    #displaynetwork\n",
    "    if display:\n",
    "       disp.plot()\n",
    "    #store the values to an array, Accuracy, Precision, Recall, F1\n",
    "    Performance = [acc,precision,recall,f1]\n",
    "    return Performance\n",
    "# KNN using 1 neighbor\n",
    "KNN1 = KNN_model(X_train_z,Y_train_z,1,X_val_z,Y_val_z,False)\n",
    "# KNN using 3 neighbors\n",
    "KNN3 = KNN_model(X_train_z,Y_train_z,3,X_val_z,Y_val_z,False)\n",
    "# KNN using 5 neighbors\n",
    "KNN5 = KNN_model(X_train_z,Y_train_z,5,X_val_z,Y_val_z,False)\n",
    "#storing all the performances from the KNN models\n",
    "results_KNN = pd.DataFrame([KNN1,KNN3,KNN5],['1NN','3NN','5NN'],['Accuracy','Precision','Recall','F1'])\n",
    "results_KNN.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#SVM model\n",
    "def SVM_model(x,y,kernaltype,x_val,y_val,display):\n",
    "    model = sklearn.svm.SVC(kernel=kernaltype)\n",
    "    model.fit(x,y)\n",
    "    #Test model on validation data\n",
    "    prediction = model.predict(x_val)\n",
    "    true = y_val\n",
    "    acc = (sklearn.metrics.accuracy_score(true,prediction))\n",
    "    precision = (sklearn.metrics.precision_score(true,prediction))\n",
    "    recall = (sklearn.metrics.recall_score(true,prediction))\n",
    "    f1 = (sklearn.metrics.f1_score(true,prediction))\n",
    "    cm = sklearn.metrics.confusion_matrix(true,prediction,labels=model.classes_)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(cm,display_labels=model.classes_)\n",
    "    #displaynetwork\n",
    "    if display:\n",
    "       disp.plot()\n",
    "    #store the values to an array, Accuracy, Precision, Recall, F1\n",
    "    Performance = [acc,precision,recall,f1]\n",
    "    return Performance\n",
    "#SVM model using rbf\n",
    "SVM_rbf_model = SVM_model(X_train_z,Y_train_z,'rbf',X_val_z,Y_val_z,False)\n",
    "#SVM model using linear\n",
    "SVM_linear_model = SVM_model(X_train_z,Y_train_z,'linear',X_val_z,Y_val_z,False)\n",
    "#SVM model using poly\n",
    "SVM_poly_model = SVM_model(X_train_z,Y_train_z,'poly',X_val_z,Y_val_z,False)\n",
    "#storing all the performances from the SVM models\n",
    "results_SVM = pd.DataFrame([SVM_rbf_model,SVM_linear_model,SVM_poly_model],['SVM_rbf','SVM_linear','SVM_poly'],['Accuracy','Precision','Recall','F1'])\n",
    "results_SVM.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Decision Tree Model\n",
    "def DecisionTree_model(x,y,classtype,x_val,y_val,display):\n",
    "    model = sklearn.tree.DecisionTreeClassifier(criterion=classtype)\n",
    "    model.fit(x,y)\n",
    "    #Test model on validation data\n",
    "    prediction = model.predict(x_val)\n",
    "    true = y_val\n",
    "    acc = (sklearn.metrics.accuracy_score(true,prediction))\n",
    "    precision = (sklearn.metrics.precision_score(true,prediction))\n",
    "    recall = (sklearn.metrics.recall_score(true,prediction))\n",
    "    f1 = (sklearn.metrics.f1_score(true,prediction))\n",
    "    cm = sklearn.metrics.confusion_matrix(true,prediction,labels=model.classes_)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(cm,display_labels=model.classes_)\n",
    "    #displaynetwork\n",
    "    if display:\n",
    "       disp.plot()\n",
    "    #store the values to an array, Accuracy, Precision, Recall, F1\n",
    "    Performance = [acc,precision,recall,f1]\n",
    "    return Performance\n",
    "#Decision Tree Model using gini\n",
    "DecisionTree_gini_model = DecisionTree_model(X_train_z,Y_train_z,'gini',X_val_z,Y_val_z,False)\n",
    "#Decision Tree Model using entropy\n",
    "DecisionTree_entropy_model = DecisionTree_model(X_train_z,Y_train_z,'entropy',X_val_z,Y_val_z,False)\n",
    "#storing all the performances from the Decision Tree models\n",
    "results_DecisionTree = pd.DataFrame([DecisionTree_gini_model,DecisionTree_entropy_model],['DT_gini','DT_entropy'],['Accuracy','Precision','Recall','F1'])\n",
    "results_DecisionTree.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "def Logistic_Regression_model(x,y,penaltytype,solvertype,x_val,y_val,display):\n",
    "    model = sklearn.linear_model.LogisticRegression(penalty=penaltytype,solver=solvertype)\n",
    "    model.fit(x,y)\n",
    "    #Test model on validation data\n",
    "    prediction = model.predict(x_val)\n",
    "    true = y_val\n",
    "    #evaluate the performance of the model\n",
    "    acc = (sklearn.metrics.accuracy_score(true,prediction))\n",
    "    precision = (sklearn.metrics.precision_score(true,prediction))\n",
    "    recall = (sklearn.metrics.recall_score(true,prediction))\n",
    "    f1 = (sklearn.metrics.f1_score(true,prediction))\n",
    "    #get the confusion matrix information\n",
    "    cm = sklearn.metrics.confusion_matrix(true,prediction,labels=model.classes_)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(cm,display_labels=model.classes_)\n",
    "    #displaynetwork\n",
    "    if display:\n",
    "       disp.plot()\n",
    "    #store the values to an array, Accuracy, Precision, Recall, F1\n",
    "    Performance = [acc,precision,recall,f1]\n",
    "    return Performance\n",
    "#Decision Tree Model using gini\n",
    "Logistic_Regression_l1_model = Logistic_Regression_model(X_train_z,Y_train_z,'l1','liblinear',X_val_z,Y_val_z,False)\n",
    "#Decision Tree Model using entropy\n",
    "Logistic_Regression_l2_model = Logistic_Regression_model(X_train_z,Y_train_z,'l2','lbfgs',X_val_z,Y_val_z,False)\n",
    "#storing all the performances from the Decision Tree models\n",
    "results_Logistic_Regression = pd.DataFrame([Logistic_Regression_l1_model,Logistic_Regression_l2_model],['LogRegr_l1','LogRegr_l2'],['Accuracy','Precision','Recall','F1'])\n",
    "results_Logistic_Regression.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Pool all results together into one table and display the table\n",
    "\n",
    "#concat all the result dataframes together to one\n",
    "Results_all_models = pd.concat([results_KNN,results_SVM,results_DecisionTree,results_Logistic_Regression],axis=0)\n",
    "print(\"Full List of Results from Training:\\n\")\n",
    "print(Results_all_models)\n",
    "print(f\"\\n\\n\")\n",
    "#sort the model from least to greatest F1 score value\n",
    "Results_all_models_sorted = Results_all_models.sort_values(['F1'],ascending=[False])\n",
    "print(\"Full List of Results from Training Sorted by F1 Value:\\n\")\n",
    "print(Results_all_models_sorted)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test the performance of the model on the highest performing F1 value model (KNN = 1NN)\n",
    "#get the network again, give it the test data instead of validation data\n",
    "def KNN_model_best(x,y,N,x_val,y_val,display):\n",
    "    model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=N)\n",
    "    model.fit(x,y)\n",
    "    #Test model on validation data\n",
    "    sklearn.neighbors.KNeighborsClassifier(n_neighbors = N)\n",
    "    prediction = model.predict(x_val)\n",
    "    true = y_val\n",
    "    acc = (sklearn.metrics.accuracy_score(true,prediction))\n",
    "    precision = (sklearn.metrics.precision_score(true,prediction))\n",
    "    recall = (sklearn.metrics.recall_score(true,prediction))\n",
    "    f1 = (sklearn.metrics.f1_score(true,prediction))\n",
    "    cm = sklearn.metrics.confusion_matrix(true,prediction,labels=model.classes_)\n",
    "    print(\"Confusion Matrix Values\")\n",
    "    print(cm)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(cm,display_labels=model.classes_)\n",
    "    #displaynetwork\n",
    "    if display:\n",
    "       disp.plot(cmap='copper')\n",
    "    #store the values to an array, Accuracy, Precision, Recall, F1\n",
    "    Performance = [acc,precision,recall,f1]\n",
    "    return Performance\n",
    "# KNN using 1 neighbor\n",
    "KNN1_bestmodel = KNN_model_best(X_train_z,Y_train_z,1,X_test_z,Y_test_z,True)\n",
    "results_Best_model = pd.DataFrame([KNN1_bestmodel],['1NN'],['Accuracy','Precision','Recall','F1'])\n",
    "print(f\"\\nPerformance of the best network using the test data:\\n\")\n",
    "print(results_Best_model)\n",
    "print(f\"\\nConfusion Matrix Values by Type:\\n\")\n",
    "print(f\"True Positive (TP) = 540\")\n",
    "print(f\"True Negative (TN) = 209\")\n",
    "print(f\"False Positive (FP) = 133\")\n",
    "print(f\"False Negative (FN) = 98\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do Z Normalization on the Training, Validation, and Testing Data sets, but not to the classification classes\n",
    "#define the function to be used\n",
    "def Z_Normalization(arr):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(arr)\n",
    "    scaler.mean_\n",
    "    new_arr = scaler.transform(arr)\n",
    "    return new_arr\n",
    "#Train Data\n",
    "X_train_z = Z_Normalization(x_train)\n",
    "Y_train_z = y_train\n",
    "#Validation data\n",
    "X_val_z = Z_Normalization(x_val)\n",
    "Y_val_z = y_val\n",
    "#Test data\n",
    "X_test_z = Z_Normalization(x_test)\n",
    "Y_test_z = y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sklearn KNN model\n",
    "def KNN_model(x,y,N,x_val,y_val,display):\n",
    "    model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=N)\n",
    "    model.fit(x,y)\n",
    "    #Test model on validation data\n",
    "    sklearn.neighbors.KNeighborsClassifier(n_neighbors = N)\n",
    "    prediction = model.predict(x_val)\n",
    "    true = y_val\n",
    "    acc = (sklearn.metrics.accuracy_score(true,prediction))\n",
    "    precision = (sklearn.metrics.precision_score(true,prediction))\n",
    "    recall = (sklearn.metrics.recall_score(true,prediction))\n",
    "    f1 = (sklearn.metrics.f1_score(true,prediction))\n",
    "    cm = sklearn.metrics.confusion_matrix(true,prediction,labels=model.classes_)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(cm,display_labels=model.classes_)\n",
    "    #displaynetwork\n",
    "    if display:\n",
    "       disp.plot()\n",
    "    #store the values to an array, Accuracy, Precision, Recall, F1\n",
    "    Performance = [acc,precision,recall,f1]\n",
    "    return Performance\n",
    "# KNN using 1 neighbor\n",
    "KNN1 = KNN_model(X_train_z,Y_train_z,1,X_val_z,Y_val_z,False)\n",
    "# KNN using 3 neighbors\n",
    "KNN3 = KNN_model(X_train_z,Y_train_z,3,X_val_z,Y_val_z,False)\n",
    "# KNN using 5 neighbors\n",
    "KNN5 = KNN_model(X_train_z,Y_train_z,5,X_val_z,Y_val_z,False)\n",
    "#storing all the performances from the KNN models\n",
    "results_KNN = pd.DataFrame([KNN1,KNN3,KNN5],['1NN','3NN','5NN'],['Accuracy','Precision','Recall','F1'])\n",
    "results_KNN.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#SVM model\n",
    "def SVM_model(x,y,kernaltype,x_val,y_val,display):\n",
    "    model = sklearn.svm.SVC(kernel=kernaltype)\n",
    "    model.fit(x,y)\n",
    "    #Test model on validation data\n",
    "    prediction = model.predict(x_val)\n",
    "    true = y_val\n",
    "    acc = (sklearn.metrics.accuracy_score(true,prediction))\n",
    "    precision = (sklearn.metrics.precision_score(true,prediction))\n",
    "    recall = (sklearn.metrics.recall_score(true,prediction))\n",
    "    f1 = (sklearn.metrics.f1_score(true,prediction))\n",
    "    cm = sklearn.metrics.confusion_matrix(true,prediction,labels=model.classes_)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(cm,display_labels=model.classes_)\n",
    "    #displaynetwork\n",
    "    if display:\n",
    "       disp.plot()\n",
    "    #store the values to an array, Accuracy, Precision, Recall, F1\n",
    "    Performance = [acc,precision,recall,f1]\n",
    "    return Performance\n",
    "#SVM model using rbf\n",
    "SVM_rbf_model = SVM_model(X_train_z,Y_train_z,'rbf',X_val_z,Y_val_z,False)\n",
    "#SVM model using linear\n",
    "SVM_linear_model = SVM_model(X_train_z,Y_train_z,'linear',X_val_z,Y_val_z,False)\n",
    "#SVM model using poly\n",
    "SVM_poly_model = SVM_model(X_train_z,Y_train_z,'poly',X_val_z,Y_val_z,False)\n",
    "#storing all the performances from the SVM models\n",
    "results_SVM = pd.DataFrame([SVM_rbf_model,SVM_linear_model,SVM_poly_model],['SVM_rbf','SVM_linear','SVM_poly'],['Accuracy','Precision','Recall','F1'])\n",
    "results_SVM.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Decision Tree Model\n",
    "def DecisionTree_model(x,y,classtype,x_val,y_val,display):\n",
    "    model = sklearn.tree.DecisionTreeClassifier(criterion=classtype)\n",
    "    model.fit(x,y)\n",
    "    #Test model on validation data\n",
    "    prediction = model.predict(x_val)\n",
    "    true = y_val\n",
    "    acc = (sklearn.metrics.accuracy_score(true,prediction))\n",
    "    precision = (sklearn.metrics.precision_score(true,prediction))\n",
    "    recall = (sklearn.metrics.recall_score(true,prediction))\n",
    "    f1 = (sklearn.metrics.f1_score(true,prediction))\n",
    "    cm = sklearn.metrics.confusion_matrix(true,prediction,labels=model.classes_)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(cm,display_labels=model.classes_)\n",
    "    #displaynetwork\n",
    "    if display:\n",
    "       disp.plot()\n",
    "    #store the values to an array, Accuracy, Precision, Recall, F1\n",
    "    Performance = [acc,precision,recall,f1]\n",
    "    return Performance\n",
    "#Decision Tree Model using gini\n",
    "DecisionTree_gini_model = DecisionTree_model(X_train_z,Y_train_z,'gini',X_val_z,Y_val_z,False)\n",
    "#Decision Tree Model using entropy\n",
    "DecisionTree_entropy_model = DecisionTree_model(X_train_z,Y_train_z,'entropy',X_val_z,Y_val_z,False)\n",
    "#storing all the performances from the Decision Tree models\n",
    "results_DecisionTree = pd.DataFrame([DecisionTree_gini_model,DecisionTree_entropy_model],['DT_gini','DT_entropy'],['Accuracy','Precision','Recall','F1'])\n",
    "results_DecisionTree.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Logistic Regression Model\n",
    "def Logistic_Regression_model(x,y,penaltytype,solvertype,x_val,y_val,display):\n",
    "    model = sklearn.linear_model.LogisticRegression(penalty=penaltytype,solver=solvertype)\n",
    "    model.fit(x,y)\n",
    "    #Test model on validation data\n",
    "    prediction = model.predict(x_val)\n",
    "    true = y_val\n",
    "    #evaluate the performance of the model\n",
    "    acc = (sklearn.metrics.accuracy_score(true,prediction))\n",
    "    precision = (sklearn.metrics.precision_score(true,prediction))\n",
    "    recall = (sklearn.metrics.recall_score(true,prediction))\n",
    "    f1 = (sklearn.metrics.f1_score(true,prediction))\n",
    "    #get the confusion matrix information\n",
    "    cm = sklearn.metrics.confusion_matrix(true,prediction,labels=model.classes_)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(cm,display_labels=model.classes_)\n",
    "    #displaynetwork\n",
    "    if display:\n",
    "       disp.plot()\n",
    "    #store the values to an array, Accuracy, Precision, Recall, F1\n",
    "    Performance = [acc,precision,recall,f1]\n",
    "    return Performance\n",
    "#Decision Tree Model using gini\n",
    "Logistic_Regression_l1_model = Logistic_Regression_model(X_train_z,Y_train_z,'l1','liblinear',X_val_z,Y_val_z,False)\n",
    "#Decision Tree Model using entropy\n",
    "Logistic_Regression_l2_model = Logistic_Regression_model(X_train_z,Y_train_z,'l2','lbfgs',X_val_z,Y_val_z,False)\n",
    "#storing all the performances from the Decision Tree models\n",
    "results_Logistic_Regression = pd.DataFrame([Logistic_Regression_l1_model,Logistic_Regression_l2_model],['LogRegr_l1','LogRegr_l2'],['Accuracy','Precision','Recall','F1'])\n",
    "results_Logistic_Regression.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Pool all results together into one table and display the table\n",
    "\n",
    "#concat all the result dataframes together to one\n",
    "Results_all_models = pd.concat([results_KNN,results_SVM,results_DecisionTree,results_Logistic_Regression],axis=0)\n",
    "print(\"Full List of Results from Training:\\n\")\n",
    "print(Results_all_models)\n",
    "print(f\"\\n\\n\")\n",
    "#sort the model from least to greatest F1 score value\n",
    "Results_all_models_sorted = Results_all_models.sort_values(['F1'],ascending=[False])\n",
    "print(\"Full List of Results from Training Sorted by F1 Value:\\n\")\n",
    "print(Results_all_models_sorted)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test the performance of the model on the highest performing F1 value model (KNN = 1NN)\n",
    "#get the network again, give it the test data instead of validation data\n",
    "def KNN_model_best(x,y,N,x_val,y_val,display):\n",
    "    model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=N)\n",
    "    model.fit(x,y)\n",
    "    #Test model on validation data\n",
    "    sklearn.neighbors.KNeighborsClassifier(n_neighbors = N)\n",
    "    prediction = model.predict(x_val)\n",
    "    true = y_val\n",
    "    acc = (sklearn.metrics.accuracy_score(true,prediction))\n",
    "    precision = (sklearn.metrics.precision_score(true,prediction))\n",
    "    recall = (sklearn.metrics.recall_score(true,prediction))\n",
    "    f1 = (sklearn.metrics.f1_score(true,prediction))\n",
    "    cm = sklearn.metrics.confusion_matrix(true,prediction,labels=model.classes_)\n",
    "    print(\"Confusion Matrix Values\")\n",
    "    print(cm)\n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(cm,display_labels=model.classes_)\n",
    "    #displaynetwork\n",
    "    if display:\n",
    "       disp.plot(cmap='copper')\n",
    "    #store the values to an array, Accuracy, Precision, Recall, F1\n",
    "    Performance = [acc,precision,recall,f1]\n",
    "    return Performance\n",
    "# KNN using 1 neighbor\n",
    "KNN1_bestmodel = KNN_model_best(X_train_z,Y_train_z,1,X_test_z,Y_test_z,True)\n",
    "results_Best_model = pd.DataFrame([KNN1_bestmodel],['1NN'],['Accuracy','Precision','Recall','F1'])\n",
    "print(f\"\\nPerformance of the best network using the test data:\\n\")\n",
    "print(results_Best_model)\n",
    "print(f\"\\nConfusion Matrix Values by Type:\\n\")\n",
    "print(f\"True Positive (TP) = 540\")\n",
    "print(f\"True Negative (TN) = 209\")\n",
    "print(f\"False Positive (FP) = 133\")\n",
    "print(f\"False Negative (FN) = 98\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}